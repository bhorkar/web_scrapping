{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The standard library modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# The wget module\n",
    "import wget\n",
    "\n",
    "# The BeautifulSoup module\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# The selenium module\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import stem\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "from stem.connection import connect\n",
    "import time\n",
    "\n",
    "executable_path='/home/bhorkar/node_modules/phantomjs-prebuilt/lib/phantom/bin//phantomjs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Python script to connect to Tor via Stem and Privoxy, requesting a new connection (hence a new IP as well) as desired.\n",
    "'''\n",
    "\n",
    "import stem\n",
    "import stem.connection\n",
    "\n",
    "import time\n",
    "import urllib2\n",
    "\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "\n",
    "# initialize some HTTP headers\n",
    "# for later usage in URL requests\n",
    "user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "headers={'User-Agent':user_agent}\n",
    "\n",
    "# initialize some\n",
    "# holding variables\n",
    "global oldIP\n",
    "oldIP = \"0.0.0.0\"\n",
    "newIP = \"0.0.0.0\"\n",
    "\n",
    "# how many IP addresses\n",
    "# through which to iterate?\n",
    "nbrOfIpAddresses = 1\n",
    "\n",
    "# seconds between\n",
    "# IP address checks\n",
    "secondsBetweenChecks = 2\n",
    "\n",
    "# request a URL \n",
    "def request(url):\n",
    "    # communicate with TOR via a local proxy (privoxy)\n",
    "    def _set_urlproxy():\n",
    "        proxy_support = urllib2.ProxyHandler({\"http\" : \"127.0.0.1:8118\"})\n",
    "        opener = urllib2.build_opener(proxy_support)\n",
    "        urllib2.install_opener(opener)\n",
    "\n",
    "    # request a URL\n",
    "    # via the proxy\n",
    "    _set_urlproxy()\n",
    "    request=urllib2.Request(url, None, headers)\n",
    "    return urllib2.urlopen(request).read()\n",
    "\n",
    "# signal TOR for a new connection \n",
    "def renew_connection():\n",
    "    with Controller.from_port(port = 9051) as controller:\n",
    "        controller.authenticate(password = 'garbage123')\n",
    "        controller.signal(Signal.NEWNYM)\n",
    "        controller.close()\n",
    "        \n",
    "\n",
    "\n",
    "ua_list = []\n",
    "with open('ualist.txt') as ua:\n",
    "    for line in ua:\n",
    "        ua_list.append(line.strip())\n",
    "#print ua_list\n",
    "ua_list_len = len(ua_list)\n",
    "\n",
    "import random\n",
    "\n",
    "def request(url,ua_list = ua_list,ua_list_len = ua_list_len):\n",
    "    ua = ua_list[random.randint(0,ua_list_len-1)]\n",
    "    headers={'User-Agent': ua}\n",
    "    request=urllib2.Request(url, None, headers)\n",
    "    return urllib2.urlopen(request).read()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO[Fri Jun 16 07:31:52 2017]:Event loop received close message.\n",
      "185.170.41.8\n",
      "\n",
      "INFO[Fri Jun 16 07:32:08 2017]:Event loop received close message.\n",
      "91.219.237.244\n",
      "\n",
      "INFO[Fri Jun 16 07:32:09 2017]:Event loop received close message.\n",
      "91.219.237.244\n",
      "\n",
      "INFO[Fri Jun 16 07:32:10 2017]:Event loop received close message.\n",
      "91.219.237.244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from TorCtl import TorCtl\n",
    "import urllib2\n",
    " \n",
    "user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "headers={'User-Agent':user_agent}\n",
    " \n",
    "def request(url):\n",
    "    def _set_urlproxy():\n",
    "        proxy_support = urllib2.ProxyHandler({\"http\" : \"127.0.0.1:8118\"})\n",
    "        opener = urllib2.build_opener(proxy_support)\n",
    "        urllib2.install_opener(opener)\n",
    "    _set_urlproxy()\n",
    "    request=urllib2.Request(url, None, headers)\n",
    "    return urllib2.urlopen(request).read()\n",
    " \n",
    "def renew_connection():\n",
    "    conn = TorCtl.connect(controlAddr=\"127.0.0.1\", controlPort=9051, passphrase=\"garbage123\")\n",
    "    conn.send_signal(\"NEWNYM\")\n",
    "    conn.close()\n",
    " \n",
    "for i in range(0,4 ):\n",
    "    renew_connection()\n",
    "    print request(\"http://icanhazip.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# cycle through\n",
    "# the specified number\n",
    "# of IP addresses via TOR \n",
    "def changeip(newIP):\n",
    " for i in range(0, nbrOfIpAddresses):\n",
    "\n",
    "    # if it's the first pass\n",
    "    if newIP == \"0.0.0.0\":\n",
    "        # renew the TOR connection\n",
    "        oldIP = newIP\n",
    "        renew_connection()\n",
    "        # obtain the \"new\" IP address\n",
    "        newIP = request(\"http://icanhazip.com/\")\n",
    "    # otherwise\n",
    "    else:\n",
    "        # remember the\n",
    "        # \"new\" IP address\n",
    "        # as the \"old\" IP address\n",
    "        oldIP = newIP\n",
    "        # refresh the TOR connection\n",
    "        renew_connection()\n",
    "        # obtain the \"new\" IP address\n",
    "        newIP = request(\"http://icanhazip.com/\")\n",
    "        print newIP\n",
    "\n",
    "    # zero the \n",
    "    # elapsed seconds    \n",
    "    seconds = 0\n",
    "\n",
    "    # loop until the \"new\" IP address\n",
    "    # is different than the \"old\" IP address,\n",
    "    # as it may take the TOR network some\n",
    "    # time to effect a different IP address\n",
    "    while oldIP == newIP:\n",
    "        # sleep this thread\n",
    "        # for the specified duration\n",
    "        time.sleep(secondsBetweenChecks)\n",
    "        # track the elapsed seconds\n",
    "        seconds += secondsBetweenChecks\n",
    "        # obtain the current IP address\n",
    "        newIP = request(\"http://icanhazip.com/\")\n",
    "        # signal that the program is still awaiting a different IP address\n",
    "        #print (\"%d seconds elapsed awaiting a different IP address.\" % seconds)\n",
    "    # output the\n",
    "    # new IP address\n",
    " #   print (\"\")\n",
    "    print (\"newIP: %s\" % newIP)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO[Fri Jun 16 07:32:12 2017]:Event loop received close message.\n",
      "newIP: 91.219.237.244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newIP = changeip(newIP);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link = \"http://www.thebudgetbabe.com/\" # load the web page\n",
    "\n",
    "driver = webdriver.PhantomJS(executable_path = executable_path)\n",
    "# it takes forever to load the page, therefore we are setting a threshold\n",
    "driver.set_page_load_timeout(500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#driver.get(link)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pgsrc  = driver.page_source \n",
    "#soup = BeautifulSoup(pgsrc, 'html.parser')\n",
    "#print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def loading_link(driver):\n",
    " shop_urls = [];\n",
    " street_urls = [];\n",
    " shop_urls1 = [];\n",
    " items = driver.find_elements_by_xpath(\"//div[contains(@class, 'serendipity_entry_body')]\")\n",
    " for elem in items:\n",
    "   # print elem.get_attribute(\"class\")\n",
    "  img_links = elem.find_elements_by_xpath(\".//img\")\n",
    "  idx = 0;\n",
    "  for sl in img_links:\n",
    "    idx = idx + 1;\n",
    "    if len(sl.get_attribute(\"alt\")) > 0:\n",
    "        id = sl.get_attribute(\"alt\").replace(' ','')[0:10]\n",
    "        street_urls.append((id, idx, sl.get_attribute(\"src\")))\n",
    "    else: \n",
    "        #if (sl.get_attribute(\"srcset\")):\n",
    "         #   shop_urls.append((id, idx, sl.get_attribute(\"src\").split('&max')[0]))\n",
    "        continue;\n",
    "    shop_links = elem.find_elements_by_xpath(\".//a[contains(@href, 'http://rstyle.me')]\")\n",
    "    url_idx = 0;\n",
    "    for sl in shop_links:\n",
    "        url_idx = url_idx + 1;\n",
    "        shop_urls.append((id,url_idx,sl.get_attribute(\"href\")))\n",
    "    shop_links = elem.find_elements_by_xpath(\".//a[contains(@target, '_blank')]\")\n",
    "    url_idx = 0;\n",
    "    for sl in shop_links:\n",
    "        url_idx = url_idx + 1;\n",
    "        shop_urls1.append((id,url_idx,sl.get_attribute(\"href\")))\n",
    "    #print \"loading #urls\", len(shop_urls), len(street_urls)\n",
    " return shop_urls,shop_urls1,street_urls;\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://www.thebudgetbabe.com/\"\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "items = driver.find_elements_by_xpath(\"//div[contains(@class, 'serendipity_entry_body')]\")\n",
    "street_urls = [];\n",
    "shop_urls = [];\n",
    "id = '';\n",
    "for elem in items:\n",
    "   # print elem.get_attribute(\"class\")\n",
    "  img_links = elem.find_elements_by_xpath(\".//img\")\n",
    "  idx = 0;\n",
    "  for sl in img_links:\n",
    "    idx = idx + 1;\n",
    "    if len(sl.get_attribute(\"alt\")) > 0:\n",
    "        id = sl.get_attribute(\"alt\").replace(' ','')[0:10]\n",
    "        street_urls.append((id, idx, sl.get_attribute(\"src\")))\n",
    "    else:\n",
    "        pass\n",
    "       # if (sl.get_attribute(\"srcset\")):\n",
    "        #    shop_urls.append((id, idx, sl.get_attribute(\"src\").split('&max')[0]))\n",
    "    shop_links = elem.find_elements_by_xpath(\".//a[contains(@href, 'http://rstyle.me')]\")\n",
    "    url_idx = 0;\n",
    "    for sl in shop_links:\n",
    "        url_idx = url_idx + 1;\n",
    "        shop_urls.append((id,url_idx,sl.get_attribute(\"href\")))\n",
    "    shop_links = elem.find_elements_by_xpath(\".//a[contains(@target, '_blank')]\")\n",
    "    url_idx = 0;\n",
    "    for sl in shop_links:\n",
    "        url_idx = url_idx + 1;\n",
    "        shop_urls.append((id,url_idx,sl.get_attribute(\"href\")))\n",
    "\n",
    "print street_urls\n",
    "print shop_urls\n",
    "img_links = elem.find_elements_by_xpath(\".//img\")\n",
    "for sl in street_links:\n",
    "     # print sl\n",
    "      #print  sl.get_attribute(\"alt\")\n",
    "      if len(sl.get_attribute(\"alt\")) > 0:\n",
    "       print sl.get_attribute(\"src\")\n",
    "      if (sl.get_attribute(\"srcset\")):\n",
    "            print sl.get_attribute(\"src\")\n",
    "      attrs = driver.execute_script('var items = {}; for (index = 0; index < arguments[0].attributes.length; ++index) { items[arguments[0].attributes[index].name] = arguments[0].attributes[index].value }; return items;', sl)\n",
    "      print(attrs)\n",
    "shop_links  = elem.find_elements_by_xpath(\".//a[contains(@href, 'http://rstyle.me')]\")\n",
    "for sl in shop_links :\n",
    "      print sl\n",
    "      print  sl.get_attribute(\"href\")\n",
    "      attrs = driver.execute_script('var items = {}; for (index = 0; index < arguments[0].attributes.length; ++index) { items[arguments[0].attributes[index].name] = arguments[0].attributes[index].value }; return items;', sl)\n",
    "      print(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shop_urls = []\n",
    "street_urls = [];\n",
    "shop_urls1 = [];\n",
    "\n",
    "for i in xrange(1,700):\n",
    "    if i%20 == 0:\n",
    "        print i;\n",
    "        print \"loading #urls\", len(shop_urls)\n",
    "        try: \n",
    "          newIP = changeip(newIP);\n",
    "        except:\n",
    "            pass;\n",
    "    if i == 1:\n",
    "        url = \"http://www.thebudgetbabe.com/\"\n",
    "    else:\n",
    "        url = \"http://www.thebudgetbabe.com/archives/P\" + str(i) +'.html'\n",
    "    try:\n",
    "        #print url\n",
    "        driver.get(url)\n",
    "        #driver.set_page_load_timeout(50)\n",
    "        sh, sh1, st = (loading_link(driver));\n",
    "        shop_urls.extend(sh)\n",
    "        street_urls.extend(st)\n",
    "        shop_urls1.extend(sh1)\n",
    "    except:\n",
    "        print \"error\",i\n",
    "        print \"Unexpected error:\", sys.exc_info()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "print \"number of sttreet urls {} {} {}\".format(len(street_urls), len(shop_urls), i)\n",
    "with open('objs.pickle_thebudgetbabe','w') as f:\n",
    "    pickle.dump([street_urls,shop_urls,shop_urls1],f);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sttreet urls 3273 3920 \n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('objs.pickle_thebudgetbabe','r') as f:\n",
    "    [street_urls,shop_urls, shop_urls1] = pickle.load(f);\n",
    "\n",
    "print \"number of sttreet urls {} {} \".format(len(street_urls), len(shop_urls))\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "print (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import sleep \n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def job(url,directory):\n",
    "  try: \n",
    "    driver = webdriver.PhantomJS(executable_path = executable_path)\n",
    "    driver.get(url[2]);\n",
    "    slp_flt =   random.uniform(3,6)                                                                                 \n",
    "   # print 'sleeping',slp_flt                                                                                        \n",
    "    sleep(slp_flt)                                                                                                                                                                                                                                          \n",
    "    pgsrc  = driver.page_source \n",
    "    soup = BeautifulSoup(pgsrc, 'html.parser')\n",
    "    fname= directory + str(url[0]) + '_'  + str(url[1]) + '_' +  \"_image-gallery.html\" \n",
    "    outf = open(fname,\"w\")                                                                                          \n",
    "    outf.write(pgsrc.encode('utf-8'))\n",
    "    driver.close();\n",
    "    print fname \n",
    "  except:\n",
    "    pass;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./shop_2/Didyouknow_1__image-gallery.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from functools import partial\n",
    "\n",
    "#pool = Pool()\n",
    "\n",
    "directory = './shop_2/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "job(shop_urls[0],directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for url in shop_urls:\n",
    "    job(url,directory)\n",
    "#pool.map( partial(job,directory=directory), shop_urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./street_2/BellaHadid_1__image-gallery.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directory = './street_2/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for url in street_urls:\n",
    "    job(url,directory)\n",
    "    break;\n",
    "#pool.map(  partial(job, directory=directory), street_urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import wget\n",
    "\n",
    "from BeautifulSoup import BeautifulSoup as bs\n",
    "import urlparse\n",
    "from urllib2 import urlopen\n",
    "from urllib import urlretrieve\n",
    "import os\n",
    "import sys\n",
    "import socket\n",
    "\n",
    "socket.setdefaulttimeout(30)\n",
    "\n",
    "def main(url, out_folder=\"./shop/\", local = 1):\n",
    "    \"\"\"Downloads all the images at 'url' \"\"\"\n",
    "    soup = bs(open(url))\n",
    "    count = 0;\n",
    "    for image in soup.findAll(\"img\"):\n",
    "       # print image\n",
    "        count = count  + 1;\n",
    "        try: \n",
    "            \n",
    "          filename = url.split('/')[2].split('.')[0] + '_' + str(count) + '_' + '.'+image[\"src\"].split(\".\")[-1]\n",
    "         # print filename\n",
    "        except:\n",
    "            print \"image not found\", \n",
    "            continue\n",
    "        image_url = urlparse.urljoin(url, image['src'])\n",
    "        #try: \n",
    "        #print image_url\n",
    "        try: \n",
    "           #wget.download(image_url);\n",
    "           urlretrieve(image['src'],out_folder+filename)\n",
    "           continue; \n",
    "        except:\n",
    "            \n",
    "            pass\n",
    "   \n",
    "        try: \n",
    "           #wget.download(image_url);\n",
    "           urlretrieve(image_url,out_folder+filename)\n",
    "        except:\n",
    "            pass\n",
    "   \n",
    "\n",
    "          \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "out_folder = \"./shop_1/\"\n",
    "urls = glob.glob(out_folder+'/*.html');\n",
    "print urls[0]\n",
    "main(urls[0], out_folder)\n",
    "main(urls[1], out_folder)\n",
    "main(urls[2], out_folder)\n",
    "\n",
    "for url in urls:\n",
    "    main(url, out_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#pool = Pool()\n",
    "#pool.map(main, urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./street_1/attachment_120371_1__image-gallery.html\n",
      "image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found image not found"
     ]
    }
   ],
   "source": [
    "\n",
    "out_folder = \"./street_1/\"\n",
    "urls = glob.glob(out_folder+'/*.html');\n",
    "print urls[0]\n",
    "main(urls[0], out_folder)\n",
    "\n",
    "\n",
    "#pool = Pool()\n",
    "#pool.map(main, urls)\n",
    "for url in urls:\n",
    "    main(url, out_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for img in soup2.findAll('a href'):\n",
    "#    print img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import re\n",
    "#urls = re.findall(r'href=[\\'\"]?(http://rstyle.me[^\\'\" >]+)', soup2.prettify())\n",
    "#print ', '.join(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
